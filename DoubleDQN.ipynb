{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "\n",
    "GAME = 'LunarLander-v2'\n",
    "env = gym.make(GAME)\n",
    "\n",
    "LOG_PATH = 'log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIONS = env.action_space.n\n",
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATE_SHAPE = env.observation_space.shape\n",
    "STATE_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_SHAPE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super(DQN, self).__init__()\n",
    "        # layers\n",
    "        self.fc1 = nn.Linear(in_shape, FC_SHAPE)\n",
    "        self.fc2 = nn.Linear(FC_SHAPE, FC_SHAPE)  # hidden 1\n",
    "        self.fc3 = nn.Linear(FC_SHAPE, FC_SHAPE)  # hidden 2\n",
    "        self.out = nn.Linear(FC_SHAPE, out_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "TAU = 1e-3                  # soft-update step\n",
    "SOFT_UPDATE = True\n",
    "\n",
    "EPSILON_DECAY = 0.996       # at each step\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.99                # discount\n",
    "SAVE_FREQUENCY = 50         # frequency(in episodes) in which the model is saved\n",
    "MIN_EXP_REPLAY = 1000       # min. size of exp. replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = DQN(STATE_SHAPE[0], ACTIONS)\n",
    "target_model = DQN(STATE_SHAPE[0], ACTIONS)\n",
    "optimizer    = Adam(policy_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# set target_model parameters equal to model ones\n",
    "target_model.load_state_dict(policy_model.state_dict());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_parameters(soft=False):\n",
    "    if soft:\n",
    "        # target_weights = policy_weights * tau + target_weights*(1-tau)\n",
    "        [target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data) \\\n",
    "         for target_param, local_param in zip(target_model.parameters(), policy_model.parameters())]\n",
    "    else:\n",
    "        target_model.load_state_dict(policy_model.state_dict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(deque):\n",
    "    def sample(self, n):\n",
    "        indexes = np.random.choice(len(self), size=n, replace=False)\n",
    "        minibatch = [self[i] for i in indexes]\n",
    "        return minibatch\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_replay = Memory(maxlen=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = torch.tensor(GAMMA)\n",
    "def update_step(experiences):\n",
    "    cols = len(experiences[0])\n",
    "    # separate each column of the batch\n",
    "    states, actions, next_states, rewards, dones = [torch.tensor(np.array([experiences[i][k] for i in range(BATCH_SIZE)])) for k in range(cols)]\n",
    "    \n",
    "    # mask helps to select desired actions only\n",
    "    mask = F.one_hot(actions.to(torch.int64), num_classes=ACTIONS)\n",
    "    \n",
    "    # Loss = R+ γQ(s′,argmaxQ(s′,a;θ);θ−) - Q(s,a;θ)\n",
    "    # s: actual state\n",
    "    # a: action taken\n",
    "    # s′: next_state (resulting state when performing action a in state s)\n",
    "    # θ: policy_model parameters\n",
    "    # θ-: target_model parameters\n",
    "    \n",
    "    # Q(s,a;θ)\n",
    "    q_values_policy = (policy_model(states)*mask).sum(1)\n",
    "    \n",
    "    # argmaxQ(s′,a;θ) is an ACTION\n",
    "    # since Q(s′,a;θ) computed by the model\n",
    "    # return the q_values of EACH ACTION \n",
    "    # that can be chosen in that state.\n",
    "    # Detach is applied to avoid computing gradients\n",
    "    policy_actions = policy_model(next_states).detach().argmax(1).unsqueeze(-1)\n",
    "\n",
    "    # Q(s′,a;θ−)\n",
    "    # target networks evaluates policy_actions,\n",
    "    # i.e. actions selected by policy network.\n",
    "    # Detach is applied to avoid computing gradients\n",
    "    q_values_next_target = target_model(next_states).detach().gather(1, policy_actions).flatten()\n",
    "\n",
    "    # Loss = R+ γQ(s′,argmaxQ(s′,a;θ);θ−) - Q(s,a;θ)\n",
    "    td_errors = rewards + GAMMA*q_values_next_target*(~dones) - q_values_policy\n",
    "\n",
    "    # MSE Error (loss over the batch)\n",
    "    loss = (0.5 * (td_errors ** 2)).mean()\n",
    "        \n",
    "    # optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update target_model parameters\n",
    "    update_target_parameters(SOFT_UPDATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy policy\n",
    "def get_action(state, epsilon):\n",
    "    # np.random.uniform() output a random float\n",
    "    # number between 0 and 1\n",
    "    if np.random.uniform() < epsilon:    \n",
    "        # exploring\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # exploiting\n",
    "        q_vals = policy_model(torch.tensor(state))\n",
    "        # select action with higher q_values\n",
    "        action = np.argmax(q_vals.detach().numpy())\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_experience_replay():\n",
    "    state = env.reset()\n",
    "    \n",
    "    # fill experience replay buffer with random experiences\n",
    "    for i in  tqdm(range(MIN_EXP_REPLAY)):\n",
    "        # random action\n",
    "        action = env.action_space.sample()\n",
    "        # perform random action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # append to experience_replay\n",
    "        experience_replay.append((state, action ,next_state, reward, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # percentage of fullness\n",
    "        tmp = len(experience_replay)/MIN_EXP_REPLAY*100\n",
    "        if tmp % 10 == 0:  print(f\"Exp. replay {tmp}%\")\n",
    "            \n",
    "        if done: state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(fname):\n",
    "    torch.save(policy_model.state_dict(), f\"model/{fname}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 860/1000 [00:00<00:00, 4363.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp. replay 10.0%\n",
      "Exp. replay 20.0%\n",
      "Exp. replay 30.0%\n",
      "Exp. replay 40.0%\n",
      "Exp. replay 50.0%\n",
      "Exp. replay 60.0%\n",
      "Exp. replay 70.0%\n",
      "Exp. replay 80.0%\n",
      "Exp. replay 90.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 4242.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp. replay 100.0%\n",
      "Episode 1\t Epsilon 0.66\t Ep.Reward -106.66\t Mean Rew. -106.66\n",
      "New best score: -106.66. Saving\n",
      "Episode 2\t Epsilon 0.44\t Ep.Reward -176.44\t Mean Rew. -141.55\n",
      "Episode 3\t Epsilon 0.19\t Ep.Reward -237.97\t Mean Rew. -173.69\n",
      "Episode 4\t Epsilon 0.12\t Ep.Reward -116.32\t Mean Rew. -159.35\n",
      "Episode 5\t Epsilon 0.09\t Ep.Reward -306.76\t Mean Rew. -188.83\n",
      "Episode 6\t Epsilon 0.06\t Ep.Reward -334.61\t Mean Rew. -213.13\n",
      "Episode 7\t Epsilon 0.03\t Ep.Reward -79.76\t Mean Rew. -194.07\n",
      "New best score: -79.76. Saving\n",
      "Episode 8\t Epsilon 0.02\t Ep.Reward -90.22\t Mean Rew. -181.09\n",
      "Episode 9\t Epsilon 0.01\t Ep.Reward -44.49\t Mean Rew. -165.91\n",
      "New best score: -44.49. Saving\n",
      "Episode 10\t Epsilon 0.01\t Ep.Reward -179.60\t Mean Rew. -167.28\n",
      "Episode 11\t Epsilon 0.01\t Ep.Reward -90.98\t Mean Rew. -160.35\n",
      "Episode 12\t Epsilon 0.01\t Ep.Reward -99.05\t Mean Rew. -155.24\n",
      "Episode 13\t Epsilon 0.01\t Ep.Reward -274.34\t Mean Rew. -164.40\n",
      "Episode 14\t Epsilon 0.01\t Ep.Reward -285.59\t Mean Rew. -173.06\n",
      "Episode 15\t Epsilon 0.01\t Ep.Reward -81.80\t Mean Rew. -166.97\n",
      "Episode 16\t Epsilon 0.01\t Ep.Reward -83.15\t Mean Rew. -161.73\n",
      "Episode 17\t Epsilon 0.01\t Ep.Reward 13.42\t Mean Rew. -151.43\n",
      "New best score: 13.42. Saving\n",
      "Episode 18\t Epsilon 0.01\t Ep.Reward -348.44\t Mean Rew. -162.38\n",
      "Episode 19\t Epsilon 0.01\t Ep.Reward -105.95\t Mean Rew. -159.41\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 500\n",
    "\n",
    "n_episodes = 0\n",
    "epsilon = MAX_EPSILON\n",
    "episode_reward = 0\n",
    "total_rewards = []\n",
    "total_mean_rewards = []\n",
    "highest_reward = float('-inf')\n",
    "\n",
    "# filling experience replay\n",
    "if len(experience_replay) < MIN_EXP_REPLAY: fill_experience_replay()\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "try:\n",
    "    while n_episodes < MAX_EPISODES:\n",
    "\n",
    "        # action selection, according to greedy-policy\n",
    "        action = get_action(state, epsilon)\n",
    "\n",
    "        # performing the action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        # adding experience to experience replay\n",
    "        experience_replay.append((state, action, next_state, reward, done))\n",
    "\n",
    "        # learning phase\n",
    "        minibatch = experience_replay.sample(min(len(experience_replay), BATCH_SIZE))\n",
    "        update_step(minibatch)\n",
    "        \n",
    "        # decrease epsilon\n",
    "        epsilon = max(MIN_EPSILON, EPSILON_DECAY*epsilon)\n",
    "\n",
    "        # update actual state\n",
    "        state = next_state\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # end of the episode\n",
    "        if done:\n",
    "            n_episodes += 1\n",
    "            total_rewards.append(episode_reward)\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            total_mean_rewards.append(mean_reward)\n",
    "            print(\"Episode %d\\t Epsilon %.2f\\t Ep.Reward %.2f\\t Mean Rew. %.2f\" % (n_episodes, epsilon, episode_reward, mean_reward))\n",
    "\n",
    "            if episode_reward >= highest_reward:\n",
    "                print(\"New best score: %.2f. Saving\" % episode_reward)\n",
    "                save_model(\"best\")\n",
    "                highest_reward = episode_reward\n",
    "\n",
    "            if n_episodes % SAVE_FREQUENCY == 0:\n",
    "                save_model(f\"auto_{n_episodes}_episodes\")\n",
    "\n",
    "            episode_reward = 0\n",
    "            \n",
    "            state = env.reset()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted manually...\")\n",
    "    save_model(f\"manual_{int(episode_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewards.npy', 'wb') as f:\n",
    "    np.save(f, total_mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Mean rewards\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(total_mean_rewards)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aas (ipykernel)",
   "language": "python",
   "name": "aas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

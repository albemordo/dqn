{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7432bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "\n",
    "GAME = 'LunarLander-v2'\n",
    "env = gym.make(GAME)\n",
    "\n",
    "LOG_PATH = 'log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d898877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIONS = env.action_space.n\n",
    "ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f00c709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATE_SHAPE = env.observation_space.shape\n",
    "STATE_SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebf5843-db8e-4d09-9fa3-142696463c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_SHAPE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5803090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super(DQN, self).__init__()\n",
    "        # layers\n",
    "        self.fc1 = nn.Linear(in_shape, FC_SHAPE)\n",
    "        self.fc2 = nn.Linear(FC_SHAPE, FC_SHAPE)  # hidden 1\n",
    "        self.fc3 = nn.Linear(FC_SHAPE, FC_SHAPE)  # hidden 2\n",
    "        self.out = nn.Linear(FC_SHAPE, out_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac46203",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EPSILON_DECAY = 0.996       # at each step\n",
    "MAX_EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.99                # discount\n",
    "SAVE_FREQUENCY = 50         # frequency(in episodes) in which the model is saved\n",
    "MIN_EXP_REPLAY = 1000       # min. size of exp. replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85adc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(STATE_SHAPE[0], ACTIONS)\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234c1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(deque):\n",
    "    def sample(self, n):\n",
    "        indexes = np.random.choice(len(self), size=n, replace=False)\n",
    "        minibatch = [self[i] for i in indexes]\n",
    "        return minibatch\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b975fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_replay = Memory(maxlen=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3dd5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = torch.tensor(GAMMA)\n",
    "def update_step(experiences):\n",
    "    cols = len(experiences[0])\n",
    "    # separate each column of the batch\n",
    "    states, actions, next_states, rewards, dones = [np.array([experiences[i][k] for i in range(BATCH_SIZE)]) for k in range(cols)]\n",
    "    \n",
    "    # prepare tensors\n",
    "    states = torch.tensor(states)\n",
    "    next_states = torch.tensor(next_states)\n",
    "    rewards = torch.tensor(rewards)\n",
    "    dones = torch.tensor(dones)\n",
    "    actions = torch.tensor(actions)\n",
    "    \n",
    "    # mask helps to select desired actions only\n",
    "    mask = F.one_hot(actions.to(torch.int64), num_classes=ACTIONS)\n",
    "    \n",
    "    # setting gradients to zero, from now on we must only use\n",
    "    # torch functions and tensors\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward\n",
    "    q_values = (model(states)*mask).sum(1)\n",
    "\n",
    "    # forward\n",
    "    # according to algorithm, gradients are not computed for q_values_next\n",
    "    q_values_next = model(next_states).detach().max(1).values\n",
    "\n",
    "    td_errors = rewards + GAMMA*q_values_next*(~dones) - q_values\n",
    "\n",
    "    # MSE Error\n",
    "    loss = (0.5 * (td_errors ** 2)).mean()\n",
    "        \n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b1fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy policy\n",
    "def get_action(state, epsilon):\n",
    "    # np.random.uniform() output a random float\n",
    "    # number between 0 and 1\n",
    "    if np.random.uniform() < epsilon:    \n",
    "        # exploring\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # exploiting\n",
    "        q_vals = model(torch.tensor(state))\n",
    "        # select action with higher q_values\n",
    "        action = np.argmax(q_vals.detach().numpy())\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff6583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_experience_replay():\n",
    "    state = env.reset()\n",
    "    \n",
    "    # fill experience replay buffer with random experiences\n",
    "    while len(experience_replay) < MIN_EXP_REPLAY:\n",
    "        # random action\n",
    "        action = env.action_space.sample()\n",
    "        # perform random action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # append to experience_replay\n",
    "        experience_replay.append((state, action ,next_state, reward, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # percentage of fullness\n",
    "        tmp = len(experience_replay)/MIN_EXP_REPLAY*100\n",
    "        if tmp % 10 == 0:  print(f\"Exp. replay {tmp}%\")\n",
    "            \n",
    "        if done: state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11bda0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(fname):\n",
    "    torch.save(model.state_dict(), f\"model/{fname}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b1914",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp. replay 10.0%\n",
      "Exp. replay 20.0%\n",
      "Exp. replay 30.0%\n",
      "Exp. replay 40.0%\n",
      "Exp. replay 50.0%\n",
      "Exp. replay 60.0%\n",
      "Exp. replay 70.0%\n",
      "Exp. replay 80.0%\n",
      "Exp. replay 90.0%\n",
      "Exp. replay 100.0%\n",
      "Episode 1\t Epsilon 0.67\t Ep.Reward -23.94\t Mean Rew. -23.94\n",
      "New best score: -23.94. Saving\n",
      "Episode 2\t Epsilon 0.42\t Ep.Reward -236.03\t Mean Rew. -129.98\n",
      "Episode 3\t Epsilon 0.06\t Ep.Reward -72.55\t Mean Rew. -110.84\n",
      "Episode 4\t Epsilon 0.04\t Ep.Reward -163.48\t Mean Rew. -124.00\n",
      "Episode 5\t Epsilon 0.01\t Ep.Reward -133.90\t Mean Rew. -125.98\n",
      "Episode 6\t Epsilon 0.01\t Ep.Reward -116.40\t Mean Rew. -124.38\n",
      "Episode 7\t Epsilon 0.01\t Ep.Reward 85.55\t Mean Rew. -94.39\n",
      "New best score: 85.55. Saving\n",
      "Episode 8\t Epsilon 0.01\t Ep.Reward -177.47\t Mean Rew. -104.78\n",
      "Episode 9\t Epsilon 0.01\t Ep.Reward -88.35\t Mean Rew. -102.95\n",
      "Episode 10\t Epsilon 0.01\t Ep.Reward -74.69\t Mean Rew. -100.13\n",
      "Episode 11\t Epsilon 0.01\t Ep.Reward -22.42\t Mean Rew. -93.06\n",
      "Episode 12\t Epsilon 0.01\t Ep.Reward -62.89\t Mean Rew. -90.55\n",
      "Episode 13\t Epsilon 0.01\t Ep.Reward -31.87\t Mean Rew. -86.03\n",
      "Episode 14\t Epsilon 0.01\t Ep.Reward -75.02\t Mean Rew. -85.25\n",
      "Episode 15\t Epsilon 0.01\t Ep.Reward -54.75\t Mean Rew. -83.21\n",
      "Episode 16\t Epsilon 0.01\t Ep.Reward -241.75\t Mean Rew. -93.12\n",
      "Episode 17\t Epsilon 0.01\t Ep.Reward 161.36\t Mean Rew. -78.15\n",
      "New best score: 161.36. Saving\n",
      "Episode 18\t Epsilon 0.01\t Ep.Reward 226.63\t Mean Rew. -61.22\n",
      "New best score: 226.63. Saving\n",
      "Episode 19\t Epsilon 0.01\t Ep.Reward 189.65\t Mean Rew. -48.02\n",
      "Episode 20\t Epsilon 0.01\t Ep.Reward 174.51\t Mean Rew. -36.89\n",
      "Episode 21\t Epsilon 0.01\t Ep.Reward -38.35\t Mean Rew. -36.96\n",
      "Episode 22\t Epsilon 0.01\t Ep.Reward 154.19\t Mean Rew. -28.27\n",
      "Episode 23\t Epsilon 0.01\t Ep.Reward 159.85\t Mean Rew. -20.09\n",
      "Episode 24\t Epsilon 0.01\t Ep.Reward 221.61\t Mean Rew. -10.02\n",
      "Episode 25\t Epsilon 0.01\t Ep.Reward 8.41\t Mean Rew. -9.28\n",
      "Episode 26\t Epsilon 0.01\t Ep.Reward 201.93\t Mean Rew. -1.16\n",
      "Episode 27\t Epsilon 0.01\t Ep.Reward 201.38\t Mean Rew. 6.34\n",
      "Episode 28\t Epsilon 0.01\t Ep.Reward 242.99\t Mean Rew. 14.79\n",
      "New best score: 242.99. Saving\n",
      "Episode 29\t Epsilon 0.01\t Ep.Reward 240.44\t Mean Rew. 22.58\n",
      "Episode 30\t Epsilon 0.01\t Ep.Reward 200.02\t Mean Rew. 28.49\n",
      "Episode 31\t Epsilon 0.01\t Ep.Reward 203.58\t Mean Rew. 34.14\n",
      "Episode 32\t Epsilon 0.01\t Ep.Reward 204.58\t Mean Rew. 39.46\n",
      "Episode 33\t Epsilon 0.01\t Ep.Reward 50.19\t Mean Rew. 39.79\n",
      "Episode 34\t Epsilon 0.01\t Ep.Reward 267.54\t Mean Rew. 46.49\n",
      "New best score: 267.54. Saving\n",
      "Episode 35\t Epsilon 0.01\t Ep.Reward 213.67\t Mean Rew. 51.26\n",
      "Episode 36\t Epsilon 0.01\t Ep.Reward 247.51\t Mean Rew. 56.72\n",
      "Episode 37\t Epsilon 0.01\t Ep.Reward 29.57\t Mean Rew. 55.98\n",
      "Episode 38\t Epsilon 0.01\t Ep.Reward -9.26\t Mean Rew. 54.26\n",
      "Episode 39\t Epsilon 0.01\t Ep.Reward 203.54\t Mean Rew. 58.09\n",
      "Episode 40\t Epsilon 0.01\t Ep.Reward 22.07\t Mean Rew. 57.19\n",
      "Episode 41\t Epsilon 0.01\t Ep.Reward 262.22\t Mean Rew. 62.19\n",
      "Episode 42\t Epsilon 0.01\t Ep.Reward 148.97\t Mean Rew. 64.26\n",
      "Episode 43\t Epsilon 0.01\t Ep.Reward 231.88\t Mean Rew. 68.16\n",
      "Episode 44\t Epsilon 0.01\t Ep.Reward 210.78\t Mean Rew. 71.40\n",
      "Episode 45\t Epsilon 0.01\t Ep.Reward 219.81\t Mean Rew. 74.70\n",
      "Episode 46\t Epsilon 0.01\t Ep.Reward 256.58\t Mean Rew. 78.65\n",
      "Episode 47\t Epsilon 0.01\t Ep.Reward -106.91\t Mean Rew. 74.70\n"
     ]
    }
   ],
   "source": [
    "MAX_EPISODES = 500\n",
    "\n",
    "n_episodes = 0\n",
    "epsilon = MAX_EPSILON\n",
    "episode_reward = 0\n",
    "total_rewards = []\n",
    "total_mean_rewards = []\n",
    "highest_reward = float('-inf')\n",
    "\n",
    "# filling experience replay\n",
    "if len(experience_replay) < MIN_EXP_REPLAY: fill_experience_replay()\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "try:\n",
    "    while n_episodes < MAX_EPISODES:\n",
    "\n",
    "        # action selection, according to greedy-policy\n",
    "        action = get_action(state, epsilon)\n",
    "\n",
    "        # performing the action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        # adding experience to experience replay\n",
    "        experience_replay.append((state, action, next_state, reward, done))\n",
    "\n",
    "        # learning phase\n",
    "        minibatch = experience_replay.sample(min(len(experience_replay), BATCH_SIZE))\n",
    "        update_step(minibatch)\n",
    "        \n",
    "        # decrease epsilon\n",
    "        epsilon = max(MIN_EPSILON, EPSILON_DECAY*epsilon)\n",
    "\n",
    "        # update actual state\n",
    "        state = next_state\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # end of the episode\n",
    "        if done:\n",
    "            n_episodes += 1\n",
    "            total_rewards.append(episode_reward)\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            total_mean_rewards.append(mean_reward)\n",
    "            print(\"Episode %d\\t Epsilon %.2f\\t Ep.Reward %.2f\\t Mean Rew. %.2f\" % (n_episodes, epsilon, episode_reward, mean_reward))\n",
    "\n",
    "            if episode_reward >= highest_reward:\n",
    "                print(\"New best score: %.2f. Saving\" % episode_reward)\n",
    "                save_model(\"best\")\n",
    "                highest_reward = episode_reward\n",
    "\n",
    "            if n_episodes % SAVE_FREQUENCY == 0:\n",
    "                save_model(f\"auto_{n_episodes}_episodes\")\n",
    "\n",
    "            episode_reward = 0\n",
    "            \n",
    "            state = env.reset()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted manually...\")\n",
    "    save_model(f\"manual_{int(episode_reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7ba18-58a1-4a60-aec3-20e136db65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewards.npy', 'wb') as f:\n",
    "    np.save(f, total_mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a2539-fe06-4f0a-a247-cee9215233d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Mean rewards\")\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.plot(total_mean_rewards)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aas (ipykernel)",
   "language": "python",
   "name": "aas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
